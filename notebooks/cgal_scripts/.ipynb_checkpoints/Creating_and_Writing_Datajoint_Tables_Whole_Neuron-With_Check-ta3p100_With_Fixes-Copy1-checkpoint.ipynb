{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "\n",
    "#from cloudvolume import CloudVolume\n",
    "#from collections import Counter\n",
    "#from funconnect import ta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgal_Segmentation_Module as csm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the address and the username\n",
    "dj.config['database.host'] = '10.28.0.34'\n",
    "dj.config['database.user'] = 'celiib'\n",
    "dj.config['database.password'] = 'newceliipass'\n",
    "dj.config['safemode']=True\n",
    "dj.config[\"display.limit\"] = 10\n",
    "\n",
    "\n",
    "# user: celiib\n",
    "# pass: newceliipass\n",
    "# host: at-database.ad.bcm.edu\n",
    "# schemas: microns_% and celiib_%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@10.28.0.34:3306\n"
     ]
    }
   ],
   "source": [
    "schema = dj.schema('microns_ta3p100')\n",
    "ta3p100 = dj.create_virtual_module('ta3p100', 'microns_ta3p100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if temp folder doesn't exist then create it\n",
    "if (os.path.isdir(os.getcwd() + \"/temp\")) == False:\n",
    "    os.mkdir(\"temp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "##write the OFF file for the neuron\n",
    "def write_Whole_Neuron_Off_file(neuron_ID,vertices=[], triangles=[]):\n",
    "    #primary_key = dict(segmentation=1, segment_id=segment_id, decimation_ratio=0.35)\n",
    "    #vertices, triangles = (mesh_Table_35 & primary_key).fetch1('vertices', 'triangles')\n",
    "    \n",
    "    num_vertices = (len(vertices))\n",
    "    num_faces = len(triangles)\n",
    "    \n",
    "    #get the current file location\n",
    "    file_loc = pathlib.Path.cwd() / \"temp\"\n",
    "    filename = \"neuron_\" + str(neuron_ID)\n",
    "    path_and_filename = file_loc / filename\n",
    "    \n",
    "    #print(file_loc)\n",
    "    #print(path_and_filename)\n",
    "    \n",
    "    #open the file and start writing to it    \n",
    "    f = open(str(path_and_filename) + \".off\", \"w\")\n",
    "    f.write(\"OFF\\n\")\n",
    "    f.write(str(num_vertices) + \" \" + str(num_faces) + \" 0\\n\" )\n",
    "    \n",
    "    \n",
    "    #iterate through and write all of the vertices in the file\n",
    "    for verts in vertices:\n",
    "        f.write(str(verts[0]) + \" \" + str(verts[1]) + \" \" + str(verts[2])+\"\\n\")\n",
    "    \n",
    "    #print(\"Done writing verts\")\n",
    "        \n",
    "    for faces in triangles:\n",
    "        f.write(\"3 \" + str(faces[0]) + \" \" + str(faces[1]) + \" \" + str(faces[2])+\"\\n\")\n",
    "    \n",
    "    print(\"Done writing OFF file\")\n",
    "    #f.write(\"end\")\n",
    "    \n",
    "    return str(path_and_filename),str(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(schema.jobs & 'table_name=\"__component_auto_segment_whole\"').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# USE THIS FOR THE AUTOMATED PARAMETER TESTING OF THE SEGMENT\n",
    "import cgal_Segmentation_Module as csm\n",
    "import csv\n",
    "import decimal\n",
    "import time\n",
    "import os\n",
    "\n",
    "@schema\n",
    "class ComponentAutoSegmentWhole(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    # creates the labels for the mesh table\n",
    "    -> ta3p100.CleansedMesh35\n",
    "    clusters     : tinyint unsigned  #what the clustering parameter was set to\n",
    "    smoothness   : decimal(3,2)             #what the smoothness parameter was set to, number betwee 0 and 1\n",
    "    ---\n",
    "    n_triangles  : int unsigned # number of faces\n",
    "    seg_group    : longblob     # group segmentation ID's for faces from automatic CGAL segmentation\n",
    "    sdf          : longblob     #  width values for faces from from automatic CGAL segmentation\n",
    "    median_sdf   : decimal(6,5) # the median width value for the sdf values\n",
    "    mean_sdf     : decimal(6,5) #the mean width value for the sdf values\n",
    "    third_q      : decimal(6,5) #the upper quartile for the mean width values\n",
    "    ninety_perc  : decimal(6,5) #the 90th percentile for the mean width values\n",
    "    time_updated : timestamp    # the time at which the segmentation was performed\n",
    "   \n",
    "    \n",
    "   \"\"\"\n",
    "    \n",
    "    key_source = ta3p100.CleansedMesh35 & \"decimation_ratio=0.35\" #& [dict(segment_id=comp) for comp in [50467565,58045989]]#,481423,579228,694582]]\n",
    "    \n",
    "    whole_neuron_dicts = dict()\n",
    "    \n",
    "    \n",
    "    def make(self, key):\n",
    "        \n",
    "        from cgal_Segmentation_Module import cgal_segmentation\n",
    "        #key passed to function is just dictionary with the following attributes\n",
    "        \"\"\"segmentation\n",
    "        segment_id\n",
    "        decimation_ratio\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #clusters_default = 18\n",
    "        #smoothness_list = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8]\n",
    "        #cluster_list = [2,3,4,5,6]\n",
    "        \n",
    "        smoothness_list = [0.2]\n",
    "        cluster_list = [3]\n",
    "          \n",
    "        entire_neuron = (ta3p100.CleansedMesh35 & key).fetch1()\n",
    "        neuron_ID = key[\"segment_id\"]\n",
    "        component_size = int(entire_neuron[\"n_triangles\"])\n",
    "        \n",
    "        print(\"inside make function with \" + str(neuron_ID))\n",
    "        \n",
    "        total_dict = list()\n",
    "        \n",
    "        for smoothness in smoothness_list:\n",
    "            \n",
    "            for clusters in cluster_list:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                #print(str(entire_neuron[\"segment_id\"]) + \" cluster:\" + str(clusters) \n",
    "                #      + \" smoothness:\" + str(smoothness))\n",
    "\n",
    "                #generate the off file for each component\n",
    "                #what need to send them:\n",
    "                \"\"\"----From cleansed Mesh---\n",
    "                vertices\n",
    "                triangles\n",
    "                ----From component table--\n",
    "                n_vertex_indices\n",
    "                n_triangle_indices\n",
    "                vertex_indices\n",
    "                triangle_indices\"\"\"\n",
    "\n",
    "                if key['segment_id'] not in self.whole_neuron_dicts:\n",
    "                    self.whole_neuron_dicts[key['segment_id']] = (ta3p100.CleansedMesh35 & 'decimation_ratio=0.35' & dict(segment_id=key['segment_id'])).fetch1()\n",
    "                \n",
    "                \n",
    "                path_and_filename, off_file_name = write_Whole_Neuron_Off_file(neuron_ID,\n",
    "                                            self.whole_neuron_dicts[key['segment_id']][\"vertices\"],\n",
    "                                            self.whole_neuron_dicts[key['segment_id']][\"triangles\"])\n",
    "                \n",
    "                #print(\"About to start segmentation\")\n",
    "                \n",
    "                #will have generated the component file by now so now need to run the segmentation\n",
    "                csm.cgal_segmentation(path_and_filename,clusters,smoothness)\n",
    "\n",
    "                #generate the name of the files\n",
    "                smoothness_str = str(smoothness)\n",
    "                if(len(smoothness_str)<4):\n",
    "                    smoothness_str = smoothness_str + \"0\"\n",
    "                \n",
    "                \n",
    "                \n",
    "                cgal_file_name = path_and_filename + \"-cgal_\" + str(clusters) + \"_\"+str(smoothness_str)\n",
    "                group_csv_cgal_file = cgal_file_name + \".csv\"\n",
    "                sdf_csv_file_name = cgal_file_name+\"_sdf.csv\"\n",
    "\n",
    "\n",
    "                #check if file actually exists\n",
    "                import os\n",
    "                exists = os.path.isfile(group_csv_cgal_file)\n",
    "                \n",
    "                if( not exists):\n",
    "                    print(\"Segmentation not created for \" + str(off_file_name))\n",
    "                    print(\"################## \" + str(neuron_ID) + \" ##################\")\n",
    "                    \n",
    "                    #delete the off file if it exists:\n",
    "                    #off_exists = os.path.isfile(path_and_filename)\n",
    "                    print(path_and_filename + \".off\")\n",
    "                    if os.path.isfile(path_and_filename + \".off\"):\n",
    "                        os.remove(path_and_filename + \".off\")\n",
    "                else:\n",
    "\n",
    "\n",
    "                    with open(group_csv_cgal_file) as f:\n",
    "                        reader = csv.reader(f)\n",
    "                        your_list = list(reader)\n",
    "                    group_list = []\n",
    "                    for item in your_list:\n",
    "                        group_list.append(int(item[0]))\n",
    "\n",
    "                    with open(sdf_csv_file_name) as f:\n",
    "                        reader = csv.reader(f)\n",
    "                        your_list = list(reader)\n",
    "                    sdf_list = []\n",
    "                    for item in your_list:\n",
    "                        sdf_list.append(float(item[0]))\n",
    "\n",
    "                    #print(group_list)\n",
    "                    #print(sdf_list)\n",
    "\n",
    "                    #now write them to the datajoint table  \n",
    "                    #table columns for ComponentAutoSegmentation: segmentation, segment_id, decimation_ratio, compartment_type, component_index, seg_group, sdf\n",
    "                    comp_dict = dict(key,\n",
    "                                        clusters=clusters,\n",
    "                                        smoothness=smoothness,\n",
    "                                        n_triangles=component_size,\n",
    "                                        seg_group=group_list,\n",
    "                                        sdf=sdf_list,\n",
    "                                        median_sdf=np.median(sdf_list),\n",
    "                                        mean_sdf=np.mean(sdf_list),\n",
    "                                        third_q=np.percentile(sdf_list, 75),\n",
    "                                        ninety_perc=np.percentile(sdf_list, 90),\n",
    "                                        time_updated=str(datetime.datetime.now())[0:19])\n",
    "\n",
    "                    total_dict.append(comp_dict)\n",
    "                    #self.insert1(comp_dict)  #--> only inserting one at a time\n",
    "\n",
    "                    #then go and erase all of the files used: the sdf files, \n",
    "                    real_off_file_name = path_and_filename + \".off\"\n",
    "\n",
    "                    files_to_delete = [group_csv_cgal_file,sdf_csv_file_name,real_off_file_name]\n",
    "                    for fl in files_to_delete:\n",
    "                        if os.path.exists(fl):\n",
    "                            os.remove(fl)\n",
    "                        else:\n",
    "                            print(fl + \" file does not exist\")\n",
    "\n",
    "                    print(\"finished\")\n",
    "                    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "            self.insert(total_dict,skip_duplicates=True)\n",
    "            print(\"inserted all the dictionaries\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside make function with 648518346341366885\n"
     ]
    }
   ],
   "source": [
    "ComponentAutoSegmentWhole.populate(reserve_jobs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
