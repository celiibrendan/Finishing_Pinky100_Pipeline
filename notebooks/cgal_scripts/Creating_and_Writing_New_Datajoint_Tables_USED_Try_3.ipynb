{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgal_Segmentation_Module as csm\n",
    "#help(csm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1)  create the final datajoint tables to store the final spine meshs in:\\n    a. One table to store the segment data (linked to the components table)\\n    b. One table to store the labeled mesh vertices and faces of the neurons (linked to CleanseMesh)\\n2) Create the function that goes through and writes the CGAL library for all of the components\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####2 goals\n",
    "\"\"\"1)  create the final datajoint tables to store the final spine meshs in:\n",
    "    a. One table to store the segment data (linked to the components table)\n",
    "    b. One table to store the labeled mesh vertices and faces of the neurons (linked to CleanseMesh)\n",
    "2) Create the function that goes through and writes the CGAL library for all of the components\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pseduo code for function that goes through and writes the CGAL library for all of the components\\n\\n1) Recieve list of neurons to do and a flag that when set will look to\\n    another table for the clustering parameter for each neuron\\n2) Pull down the neurons mesh data from cleansedMesh\\n3) Pull down all the components with the neuron ID that have size > 100\\n4) For each component:\\n5) Generate the off file:\\n    ---------------Way I do it in the blender file----------------\\n    In load_Neuron_automatic_spine, download whole mesh\\n    a. Before create the mesh object send faces and verts to filter_verts_and_faces\\n    b. filter_verts_and_faces:\\n        downloads the indexes for the compartment\\n        Only saves off the verts that are mentioned in the indexes\\n        Only saves off the faces that are mentioned in the indexes\\n            returns them\\n    c. builds the off file by:\\n        Finding the faces that have all indices included in the verts list\\n        finish with the write_Part_Neuron_Off_file\\n    -------------------\\n    1. create the file name string: \"neuron_\" + str(segment_id) + \"_\" + str(compartment_type_name) + \"_\" + str(found_component_index)\\n    2. get the number of indices and faces\\n    3. Open them and write them to the file\\n    4. For the vertices:\\n        For each index in the vertices blob of the components table, \\n         write the coordinates in the index location of the Cleansed mesh table\\n            while creating a lookup dictionary where it has old_vert_index:new_index  (vert_lookup)\\n    5. For the faces:\\n        For each index in the faces blob of the components table,\\n             Get the new index by (vert_lookup) and save to list\\n        Write the list to the file\\n    \\n    Call the CGAL function to generate the labels\\n    String calculat the CGAL file name and the CGAL SDF value \\n    Write the two lists to the datajoint table (linked to components)\\n    '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Pseduo code for function that goes through and writes the CGAL library for all of the components\n",
    "\n",
    "1) Recieve list of neurons to do and a flag that when set will look to\n",
    "    another table for the clustering parameter for each neuron\n",
    "2) Pull down the neurons mesh data from cleansedMesh\n",
    "3) Pull down all the components with the neuron ID that have size > 100\n",
    "4) For each component:\n",
    "5) Generate the off file:\n",
    "    ---------------Way I do it in the blender file----------------\n",
    "    In load_Neuron_automatic_spine, download whole mesh\n",
    "    a. Before create the mesh object send faces and verts to filter_verts_and_faces\n",
    "    b. filter_verts_and_faces:\n",
    "        downloads the indexes for the compartment\n",
    "        Only saves off the verts that are mentioned in the indexes\n",
    "        Only saves off the faces that are mentioned in the indexes\n",
    "            returns them\n",
    "    c. builds the off file by:\n",
    "        Finding the faces that have all indices included in the verts list\n",
    "        finish with the write_Part_Neuron_Off_file\n",
    "    -------------------\n",
    "    1. create the file name string: \"neuron_\" + str(segment_id) + \"_\" + str(compartment_type_name) + \"_\" + str(found_component_index)\n",
    "    2. get the number of indices and faces\n",
    "    3. Open them and write them to the file\n",
    "    4. For the vertices:\n",
    "        For each index in the vertices blob of the components table, \n",
    "         write the coordinates in the index location of the Cleansed mesh table\n",
    "            while creating a lookup dictionary where it has old_vert_index:new_index  (vert_lookup)\n",
    "    5. For the faces:\n",
    "        For each index in the faces blob of the components table,\n",
    "             Get the new index by (vert_lookup) and save to list\n",
    "        Write the list to the file\n",
    "    \n",
    "    Call the CGAL function to generate the labels\n",
    "    String calculat the CGAL file name and the CGAL SDF value \n",
    "    Write the two lists to the datajoint table (linked to components)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "#from cloudvolume import CloudVolume\n",
    "#from collections import Counter\n",
    "#from funconnect import ta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the address and the username\n",
    "dj.config['database.host'] = '10.28.0.34'\n",
    "dj.config['database.user'] = 'celiib'\n",
    "dj.config['database.password'] = 'newceliipass'\n",
    "dj.config['safemode']=True\n",
    "dj.config[\"display.limit\"] = 20\n",
    "\n",
    "\n",
    "# user: celiib\n",
    "# pass: newceliipass\n",
    "# host: at-database.ad.bcm.edu\n",
    "# schemas: microns_% and celiib_%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting celiib@10.28.0.34:3306\n"
     ]
    }
   ],
   "source": [
    "schema = dj.schema('microns_ta3p100')\n",
    "ta3p100 = dj.create_virtual_module('ta3p100', 'microns_ta3p100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if temp folder doesn't exist then create it\n",
    "import os\n",
    "if (os.path.isdir(os.getcwd() + \"/temp\")) == False:\n",
    "    os.mkdir(\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "def generate_component_off_file(neuron_ID, compartment_type, component_id, n_vertex_indices, n_triangle_indices, \n",
    "                                vertex_indices, triangle_indices,vertices, triangles):\n",
    "    \n",
    "    #get the current file location\n",
    "    file_loc = pathlib.Path.cwd() / \"temp\"\n",
    "    filename = \"neuron_\" + str(neuron_ID) + \"_\" + str(compartment_type) + \"_\" + str(component_id)\n",
    "    path_and_filename = file_loc / filename\n",
    "    \n",
    "    #open the file and start writing to it    \n",
    "    f = open(str(path_and_filename) + \".off\", \"w\")\n",
    "    f.write(\"OFF\\n\")\n",
    "    f.write(str(n_vertex_indices) + \" \" + str(n_triangle_indices) + \" 0\\n\" )\n",
    "    \n",
    "    #start writing all of the vertices\n",
    "    \"\"\"\n",
    "        4. For the vertices:\n",
    "        For each index in the vertices blob of the components table, \n",
    "         write the coordinates in the index location of the Cleansed mesh table\n",
    "            while creating a lookup dictionary where it has old_vert_index:new_index  (vert_lookup)\n",
    "    \"\"\"       \n",
    "    verts_lookup = {}\n",
    "    for i, vin in enumerate(vertex_indices):\n",
    "        #get the coordinates of the vertex\n",
    "        coordinates = vertices[vin]\n",
    "        #write the coordinates to the off file\n",
    "        f.write(str(coordinates[0]) + \" \" + str(coordinates[1]) + \" \" + str(coordinates[2])+\"\\n\")\n",
    "        #create lookup dictionary for vertices\n",
    "        verts_lookup[vin] = i\n",
    "    \n",
    "    \"\"\"    5. For the faces:\n",
    "        For each index in the faces blob of the components table,\n",
    "             Get the new index by (vert_lookup) and save to list\n",
    "        Write the list to the file\"\"\"\n",
    "    for i,fac in enumerate(triangle_indices):\n",
    "        verts_in_fac = triangles[fac]\n",
    "        #write the verties to the off file\n",
    "        f.write(\"3 \" + str(verts_lookup[verts_in_fac[0]]) + \" \" + str(verts_lookup[verts_in_fac[1]]) + \" \" + str(verts_lookup[verts_in_fac[2]])+\"\\n\")\n",
    "        \n",
    "    \n",
    "    print(\"Done making OFF file \" + str(filename))\n",
    "    #return the name of the off file you created and the location\n",
    "    return str(path_and_filename),str(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################THE ONE WE ARE USING\n",
    "import cgal_Segmentation_Module as csm\n",
    "import csv\n",
    "import decimal\n",
    "import time\n",
    "import os\n",
    "\n",
    "@schema\n",
    "class ComponentAutoSegment(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    # creates the labels for the mesh table\n",
    "    -> ta3p100.Compartment.Component\n",
    "    clusters     : tinyint unsigned  #what the clustering parameter was set to\n",
    "    smoothness   : decimal(3,2)             #what the smoothness parameter was set to, number betwee 0 and 1\n",
    "    ---\n",
    "    n_triangles  : int unsigned # number of faces\n",
    "    seg_group    : longblob     # group segmentation ID's for faces from automatic CGAL segmentation\n",
    "    sdf          : longblob     #  width values for faces from from automatic CGAL segmentation\n",
    "    median_sdf   : decimal(6,5) # the median width value for the sdf values\n",
    "    mean_sdf     : decimal(6,5) #the mean width value for the sdf values\n",
    "    third_q      : decimal(6,5) #the upper quartile for the mean width values\n",
    "    ninety_perc  : decimal(6,5) #the 90th percentile for the mean width values\n",
    "    time_updated : timestamp    # the time at which the segmentation was performed\n",
    "   \"\"\"\n",
    "    \n",
    "    key_source = ta3p100.Compartment.Component & 'n_triangle_indices>100' & [dict(compartment_type=comp) for comp in ['Basal', 'Apical', 'Oblique', 'Dendrite']]\n",
    "    \n",
    "    whole_neuron_dicts = dict()\n",
    "    \n",
    "    def make(self, key):\n",
    "        print(\"key = \" + str(key))\n",
    "        #key passed to function is just dictionary with the following attributes\n",
    "        \"\"\"segmentation\n",
    "        segment_id\n",
    "        decimation_ratio\n",
    "        compartment_type\n",
    "        component_index\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #clusters_default = 18\n",
    "        smoothness = 0.04\n",
    "\n",
    "        Apical_Basal_Oblique_default = [12]\n",
    "        basal_big = [16]\n",
    "\n",
    "        neuron_ID = key[\"segment_id\"]\n",
    "        component = (ta3p100.Compartment.Component & key).fetch1()        \n",
    "\n",
    "        component_id = component[\"component_index\"]\n",
    "        compartment_type = component[\"compartment_type\"]\n",
    "        component_size = int(component[\"n_triangle_indices\"])\n",
    "\n",
    "        print(\"component_size = \" + str(component_size))\n",
    "\n",
    "        if (compartment_type == \"Basal\") & (component_size > 160000):\n",
    "            cluster_list = basal_big\n",
    "        else:\n",
    "            cluster_list = Apical_Basal_Oblique_default\n",
    "\n",
    "\n",
    "        for clusters in cluster_list:\n",
    "            smoothness = 0.04\n",
    "            print(str(component[\"segment_id\"]) + \" type:\" + str(component[\"compartment_type\"]) \n",
    "                      + \" index:\" + str(component[\"component_index\"]) + \" cluster:\" + str(clusters) \n",
    "                  + \" smoothness:\" + str(smoothness))\n",
    "\n",
    "            #generate the off file for each component\n",
    "            #what need to send them:\n",
    "            \"\"\"----From cleansed Mesh---\n",
    "            vertices\n",
    "            triangles\n",
    "            ----From component table--\n",
    "            n_vertex_indices\n",
    "            n_triangle_indices\n",
    "            vertex_indices\n",
    "            triangle_indices\"\"\"\n",
    "            \n",
    "            if key['segment_id'] not in self.whole_neuron_dicts:\n",
    "                self.whole_neuron_dicts[key['segment_id']] = (ta3p100.CleansedMesh & 'decimation_ratio=0.35' & dict(segment_id=key['segment_id'])).fetch1()\n",
    "            \n",
    "            path_and_filename, off_file_name = generate_component_off_file(neuron_ID, compartment_type, component_id,\n",
    "                                        component[\"n_vertex_indices\"],\n",
    "                                        component[\"n_triangle_indices\"],\n",
    "                                        component[\"vertex_indices\"],\n",
    "                                        component[\"triangle_indices\"],\n",
    "                                        self.whole_neuron_dicts[key['segment_id']][\"vertices\"],\n",
    "                                        self.whole_neuron_dicts[key['segment_id']][\"triangles\"])\n",
    "            \n",
    "            print(len(component['vertex_indices']), len(component['triangle_indices']))\n",
    "            \n",
    "            #will have generated the component file by now so now need to run the segmentation\n",
    "\n",
    "            csm.cgal_segmentation(path_and_filename,clusters,smoothness)\n",
    "\n",
    "            #generate the name of the files\n",
    "            cgal_file_name = path_and_filename + \"-cgal_\" + str(clusters) + \"_\"+str(smoothness)\n",
    "            group_csv_cgal_file = cgal_file_name + \".csv\"\n",
    "            sdf_csv_file_name = cgal_file_name+\"_sdf.csv\"\n",
    "\n",
    "            \n",
    "            try:\n",
    "                with open(group_csv_cgal_file) as f:\n",
    "                  reader = csv.reader(f)\n",
    "                  your_list = list(reader)\n",
    "                group_list = []\n",
    "                for item in your_list:\n",
    "                    group_list.append(int(item[0]))\n",
    "\n",
    "                with open(sdf_csv_file_name) as f:\n",
    "                  reader = csv.reader(f)\n",
    "                  your_list = list(reader)\n",
    "                sdf_list = []\n",
    "                for item in your_list:\n",
    "                    sdf_list.append(float(item[0]))\n",
    "            except:\n",
    "                print(\"no CGAL segmentation for \" + str(off_file_name) )\n",
    "                return\n",
    "\n",
    "            #print(group_list)\n",
    "            #print(sdf_list)\n",
    "\n",
    "            #now write them to the datajoint table  \n",
    "            #table columns for ComponentAutoSegmentation: segmentation, segment_id, decimation_ratio, compartment_type, component_index, seg_group, sdf\n",
    "#             print(dict(key,\n",
    "#                                 clusters=clusters,\n",
    "#                                 smoothness=smoothness,\n",
    "#                                 n_triangles=component[\"n_triangle_indices\"],\n",
    "#                                 seg_group=group_list,\n",
    "#                                 sdf=sdf_list,\n",
    "#                                 median_sdf=np.median(sdf_list),\n",
    "#                                 mean_sdf=np.mean(sdf_list),\n",
    "#                                 third_q=np.percentile(sdf_list, 75),\n",
    "#                                 ninety_perc=np.percentile(sdf_list, 90),\n",
    "#                                 time_updated=str(datetime.datetime.now())[0:19]))\n",
    "            \n",
    "            comp_dict = dict(key,\n",
    "                                clusters=clusters,\n",
    "                                smoothness=smoothness,\n",
    "                                n_triangles=component[\"n_triangle_indices\"],\n",
    "                                seg_group=group_list,\n",
    "                                sdf=sdf_list,\n",
    "                                median_sdf=np.median(sdf_list),\n",
    "                                mean_sdf=np.mean(sdf_list),\n",
    "                                third_q=np.percentile(sdf_list, 75),\n",
    "                                ninety_perc=np.percentile(sdf_list, 90),\n",
    "                                time_updated=str(datetime.datetime.now())[0:19])\n",
    "\n",
    "            self.insert1(comp_dict)\n",
    "\n",
    "            #then go and erase all of the files used: the sdf files, \n",
    "            real_off_file_name = path_and_filename + \".off\"\n",
    "\n",
    "            files_to_delete = [group_csv_cgal_file,sdf_csv_file_name,real_off_file_name]\n",
    "            for fl in files_to_delete:\n",
    "                if os.path.exists(fl):\n",
    "                    os.remove(fl)\n",
    "                else:\n",
    "                    print(fl + \" file does not exist\")\n",
    "\n",
    "        print(\"finished\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key = {'segmentation': 2, 'segment_id': 648518346349470171, 'decimation_ratio': Decimal('0.35'), 'compartment_type': 'Basal', 'component_index': 0}\n",
      "component_size = 257678\n",
      "648518346349470171 type:Basal index:0 cluster:16 smoothness:0.04\n",
      "Done making OFF file neuron_648518346349470171_Basal_0\n",
      "129148 257678\n",
      "finished\n",
      "--- 105.13290119171143 seconds ---\n"
     ]
    }
   ],
   "source": [
    "ComponentAutoSegment.populate(reserve_jobs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
