{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'numpy.ndarray' has no attribute '__array_function__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ee5740dbeef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatajoint\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"neuron tab labeler started new\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#setting the address and the username\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"about to connect to database\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datajoint/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# ------------- flatten import hierarchy -------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase_relation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFreeRelation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseRelation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0muser_relations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mManual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImported\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComputed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datajoint/connection.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataJointError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_error_codes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependencies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datajoint/dependencies.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataJointError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/networkx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelabel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/networkx/generators/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mego\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint_degree_seq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/networkx/generators/geometric.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcKDTree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0m_is_scipy_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_config\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mshow_numpy_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshow_numpy_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ImportError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumerictypes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_typeDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msctypeDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumeric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumeric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfromnumeric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3091\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mumath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3092\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumerictypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3093\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfromnumeric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3094\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfromnumeric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3095\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marrayprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumerictypes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumeric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0m_dt_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msctype2char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0m_NDARRAY_ARRAY_FUNCTION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_function__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_array_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'numpy.ndarray' has no attribute '__array_function__'"
     ]
    }
   ],
   "source": [
    "import datajoint as dj\n",
    "\n",
    "print(\"neuron tab labeler started new\")\n",
    "#setting the address and the username\n",
    "print(\"about to connect to database\")\n",
    "dj.config['database.host'] = '10.28.0.34'\n",
    "dj.config['database.user'] = 'celiib'\n",
    "dj.config['database.password'] = 'newceliipass'\n",
    "#will state whether words are shown or not\n",
    "dj.config['safemode']=True\n",
    "print(dj.conn(reset=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect_to_Databases()\n",
    "#create the database inside the server\n",
    "schema = dj.schema('microns_ta3',create_tables=False)\n",
    "ta3 = dj.create_virtual_module('ta3', 'microns_ta3')\n",
    "ta3p100 = dj.create_virtual_module('ta3p100', 'microns_ta3p100')\n",
    "nda = dj.create_virtual_module('nda2', 'microns_nda2')\n",
    "import numpy as np\n",
    "#reset_Scene_Variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all of the axon segment ids         \n",
    "axon_segments = (ta3p100.Neurite() & 'neurite_type=\"axon\"' & ta3p100.CurrentSegmentation).proj(presyn='segment_id')\n",
    "axon_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all of the post-synaptic segments that are significantly tuned\n",
    "\n",
    "########*******DON'T NEED TO RESTRICT BY TRACE ANYMORE ********#####  so just getting the ones with somas\n",
    "#####Needs to be the same ones we have the annotations for\n",
    "#post_trace_segments = (ta3p100.Segment & ta3p100.AllenSoma).proj(postsyn = 'segment_id')\n",
    "post_trace_segments = (ta3p100.Segment & ta3p100.ComponentLabel).proj(postsyn = 'segment_id')\n",
    "post_trace_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_ids_error = (ta3p100.SynapseCompartmentLabel() & \"segmentation=2\" & \"postsynaptic_label=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to filter away all of the errors and no color\n",
    "final_synapses_with_labels = ((ta3p100.Synapse() & post_trace_segments & axon_segments)) * ta3p100.SynapseCompartmentLabel\n",
    "final_synapses_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to filter away all of the errors and no color\n",
    "final_synapses_with_labels = ((ta3p100.Synapse() & post_trace_segments & axon_segments) - ta3p100.SynapseExclude.proj() - synapse_ids_error.proj()) * ta3p100.SynapseCompartmentLabel\n",
    "final_synapses_with_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the distance based on the table\n",
    "synapses_only_distance = (final_synapses_with_labels.proj(\"presyn\",\"postsyn\",\"synapse_x\",\"synapse_y\",\"synapse_z\",\"postsynaptic_label\"\n",
    "     ,synaptic_size=\"(syn_bbox_x2 - syn_bbox_x1)*(syn_bbox_y2- syn_bbox_y1)*(syn_bbox_z2-syn_bbox_z1)\"))\n",
    "\n",
    "synapses_only_distance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ta3.Soma()\n",
    "soma_info = ta3p100.AllenSoma().proj(\"soma_x\",\"soma_y\",\"soma_z\",postsyn = \"segment_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shorten the table by computing the distances and the norm\n",
    "complete_synapse_table = ((synapses_only_distance * soma_info * ta3.LabelKey().proj('description', postsynaptic_label='numeric')).proj(\"presyn\",\"synaptic_size\",\"postsynaptic_label\",\"description\"\n",
    "            ,x_dist='synapse_x-soma_x',y_dist='synapse_y-soma_y',\n",
    "            norm='sqrt(POWER((synapse_x-soma_x),2) + POWER((synapse_y-soma_y),2) + POWER((synapse_z-soma_z),2)  )'))\n",
    "\n",
    "\n",
    "complete_synapse_table #& 'description_spine=\"Spine\"'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_synapse_table_fixed = ((dj.U(\"segmentation\",\"synapse_id\",\"presyn\",\"postsyn\",\"postsynaptic_label\") & complete_synapse_table) \n",
    "            *  complete_synapse_table.proj(\"synaptic_size\",\"description\",\"x_dist\",\"y_dist\",\"norm\"))\n",
    "                                \n",
    "complete_synapse_table_fixed       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"   HOW THE SharedInput Table is generated\n",
    "#how they generate the SharedInput\n",
    "soma = ta3.Segment & nda.Trace\n",
    "axons = ta3.Neurite & 'neurite_type=\"axon\"'\n",
    "synapse = ta3.Synapse & axons.proj(presyn='segment_id') & complete_synapse_table_fixed\n",
    "info = (soma * soma.proj(segment_b='segment_id') & 'segment_id > segment_b' \n",
    "                & complete_synapse_table_fixed.proj(segment_b=\"postsyn\") \n",
    "                & complete_synapse_table_fixed.proj(segment_id=\"postsyn\"))\n",
    "A = (synapse & soma.proj(postsyn='segment_id')).proj(\n",
    "    'presyn', syn1='synapse_id', segment_id='postsyn')\n",
    "B = (synapse * dj.U('presyn') & soma.proj(postsyn='segment_id')).proj(\n",
    "    'presyn', syn2='synapse_id', segment_b='postsyn')\n",
    "shared = dj.U('segment_id', 'segment_b').aggr(A * B & 'segment_id > segment_b',\n",
    "                                              n_seg_shared='count(DISTINCT presyn)')\n",
    "\n",
    "\n",
    "\n",
    "shared\n",
    "other_half_table = #info - shared \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull down the data\n",
    "synapse_id,postsyn,postsynaptic_label,description,presyn = complete_synapse_table.fetch(\"synapse_id\",\n",
    "                                                        \"postsyn\",\"postsynaptic_label\",\"description\",\"presyn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add on the spine labels\n",
    "synapse_id_spine, spine_label = ta3p100.SynapseSpineLabel().fetch(\"synapse_id\",\"spine_label\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_key = ta3.LabelKey().fetch(\"description\")\n",
    "label_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "#create lookup index\n",
    "spine_lookup=dict()\n",
    "for syn_id, syn_label in tqdm(zip(synapse_id_spine,spine_label)):\n",
    "    spine_lookup[syn_id] = syn_label\n",
    "    \n",
    "#find out who is not in the list\n",
    "#print(spine_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create the extra spine list\n",
    "postsynaptic_label_spine = list()\n",
    "spine_description = list()\n",
    "spine_label_lookup = dict()\n",
    "synapse_id_spine = set(synapse_id_spine)\n",
    "for i,syn in tqdm(enumerate(synapse_id)):\n",
    "    if syn in synapse_id_spine:\n",
    "        #print(\"inside labeler\")\n",
    "        label = spine_lookup[syn]\n",
    "    else:\n",
    "        label = 0\n",
    "        \n",
    "    spine_label_lookup[syn] = label\n",
    "    postsynaptic_label_spine.append(label)\n",
    "    spine_description.append(label_key[label]) \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create pandas table from all of the data\n",
    "import pandas as pd\n",
    "postsynaptic_label_spine\n",
    "\n",
    "synapse_id,postsyn,postsynaptic_label,description,presyn\n",
    "d = dict(synapse_id=synapse_id,presyn=presyn,postsyn=postsyn,postsynaptic_label=postsynaptic_label,\n",
    "         spine_numeric=postsynaptic_label_spine,description=description,spine_description=spine_description\n",
    "          )\n",
    "\n",
    "Label_Table = pd.DataFrame(d)\n",
    "Label_Table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#query the datatable for list\n",
    "possible_spines = [\"Spine\",\"Spine Neck\",\"Spine Head\"]\n",
    "#possible_spines = [\"not_labeled\"]\n",
    "Label_Table.loc[Label_Table[\"spine_description\"].isin(possible_spines)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find the synapses without the Spines\n",
    "no_spine_table = Label_Table.loc[Label_Table[\"spine_description\"].isin([\"not_labeled\",\"Spine Neck\",\"Spine Head\"])]\n",
    "no_spine_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#pull out of the dataframe, compute the statistics and then put back in the table\n",
    "#make sure these are numpy arrays or else the where function won't work\n",
    "synapse_ids = no_spine_table['synapse_id']\n",
    "presyns = Label_Table['presyn']\n",
    "postsyns = Label_Table['postsyn']\n",
    "synapse_labels = Label_Table['postsynaptic_label']\n",
    "synapse_labels_spine = no_spine_table['spine_numeric']\n",
    "description = Label_Table['description']\n",
    "description_spine = Label_Table['spine_description']\n",
    "\n",
    "\n",
    "\n",
    "''' Way Christos aggregates:\n",
    "1) Creates counter for pre and post\n",
    "2) creates dictionary that maps the postsynaptic target to the synapse\n",
    "3) creates dictionary for synap --> label\n",
    "    create reg label, joint label, and spine label\n",
    "4) maps axons --> synapse\n",
    "'''\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#aggregate the data\n",
    "postsyns_to_synapses = dict()\n",
    "for postsyn in tqdm(postsyns_counter):\n",
    "    postsyns_to_synapses[postsyn] = synapse_ids[np.where(postsyns==postsyn)[0]]\n",
    "    \n",
    "#creates dictionary for synap --> label\n",
    "synapse_label_dict = dict()\n",
    "for synapse_id, label in tqdm(zip(*synapse_labels)):\n",
    "    synapse_label_dict[synapse_id] = label\n",
    "\n",
    "#maps axons --> synapse\n",
    "axon_to_synapse = dict()\n",
    "for synapse, presyn in tqdm(zip(synapse_ids, presyns)):\n",
    "    if presyn in axon_to_synapse:\n",
    "        axon_to_synapse[presyn].append(synapse)\n",
    "    else:\n",
    "        axon_to_synapse[presyn] = list()\n",
    "        axon_to_synapse[presyn].append(synapse)\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compartment_labels = Counter(description).keys()\n",
    "compartment_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the counter for presynapses and postysnaptic\n",
    "presyns_counter, postsyns_counter = Counter(presyns), Counter(postsyns)\n",
    "#print(presyns_counter)\n",
    "\n",
    "#creates dictionary that maps the postsynaptic target to the synapse\n",
    "\n",
    "postsyns_to_synapses = dict()\n",
    "for postsyn in tqdm(postsyns_counter.keys()):\n",
    "    postsyns_to_synapses[postsyn] = synapse_ids[np.where(postsyns==postsyn)[0]]\n",
    "    \n",
    "#creates dictionary for synap --> label\n",
    "synapse_label_dict = dict()\n",
    "for synapse_id, label in tqdm(zip(synapse_ids,synapse_labels)):\n",
    "    synapse_label_dict[synapse_id] = label\n",
    "\n",
    "#creates dictionary for synap --> spine label\n",
    "synapse_label_dict_spine = dict()\n",
    "for synapse_id, label in tqdm(zip(synapse_ids,synapse_labels_spine)):\n",
    "    synapse_label_dict_spine[synapse_id] = label\n",
    "    \n",
    "#DON'T THINK NEED JOINT LABELS!!\n",
    "#maps axons --> synapse\n",
    "axon_to_synapse = dict()\n",
    "for synapse, presyn in tqdm(zip(synapse_ids, presyns)):\n",
    "    if presyn in axon_to_synapse:\n",
    "        axon_to_synapse[presyn].append(synapse)\n",
    "    else:\n",
    "        axon_to_synapse[presyn] = list()\n",
    "        axon_to_synapse[presyn].append(synapse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The destination vectors that will be fed into the clustering machine\n",
    "\n",
    "#WHERE COMPARTMENT LABELS AND SPINES NOT SEPERATE\n",
    "\n",
    "axon_to_vector = dict()\n",
    "threshold_axon_to_vector = dict()\n",
    "axon_to_vector_spine = dict()\n",
    "threshold_axon_to_vector_spine = dict()\n",
    "\n",
    "axon_to_vector_spine_joint = dict()\n",
    "threshold_axon_to_vector_spine_joint = dict()\n",
    "\n",
    "\n",
    "\n",
    "#aggregating the data\n",
    "for presyn in tqdm(axon_to_synapse):\n",
    "    label_list = list()\n",
    "    label_list_spine = list()\n",
    "    #list to hold both compartments\n",
    "    joint_list = list()\n",
    "    for synapse in axon_to_synapse[presyn]:\n",
    "        label_list.append(synapse_label_dict[synapse])\n",
    "        label_list_spine.append(synapse_label_dict_spine[synapse])\n",
    "        joint_list.append(synapse_label_dict[synapse]*100 + synapse_label_dict_spine[synapse])\n",
    "    label_counter = Counter(label_list)\n",
    "   \n",
    "    label_counter_spine = Counter(label_list_spine)\n",
    "    label_counter_joint = Counter(joint_list)\n",
    "    #print(\"label_counter_spine[13]=\" + str(label_counter_spine[13]))\n",
    "    #print(label_counter_spine)\n",
    "    #print(label_counter)\n",
    "    \n",
    "    #the indexes here correspond to the labels b/c it is a dictionary\n",
    "    apicals   = label_counter[2]\n",
    "    basals    = label_counter[3]\n",
    "    obliques  = label_counter[4]\n",
    "    somas     = label_counter[5]\n",
    "    axons     = label_counter[6] + label_counter[7]\n",
    "    \n",
    "    spines   = label_counter_spine[14]\n",
    "    spine_heads    = label_counter_spine[13]\n",
    "    spine_neck  = label_counter_spine[15]\n",
    "    no_label     = label_counter_spine[0]\n",
    "    \n",
    "    apical_joint = (label_counter_joint[214],label_counter_joint[213],label_counter_joint[215],label_counter_joint[200])\n",
    "    basal_joint = (label_counter_joint[314],label_counter_joint[313],label_counter_joint[315],label_counter_joint[300])\n",
    "    oblique_joint = (label_counter_joint[414],label_counter_joint[413],label_counter_joint[415],label_counter_joint[400])\n",
    "    soma_joint = (label_counter_joint[514],label_counter_joint[513],label_counter_joint[515],label_counter_joint[500])\n",
    "    axon_joint = (label_counter_joint[614] +label_counter_joint[714],\n",
    "                  label_counter_joint[613] +  label_counter_joint[713],\n",
    "                  label_counter_joint[615] + label_counter_joint[715],\n",
    "                  label_counter_joint[600] + label_counter_joint[700])\n",
    "    \n",
    "    #print((apicals, basals, obliques, somas, axons))\n",
    "    #print((apical_joint + basal_joint+ oblique_joint + soma_joint + axon_joint))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(np.array((apicals, basals, obliques, somas, axons)))\n",
    "    synapse_total = sum((apicals, basals, obliques, somas, axons))\n",
    "    synapse_total_spine = sum((spines, spine_heads, spine_neck, no_label))\n",
    "    \n",
    "    if synapse_total > 0:\n",
    "        axon_to_vector[presyn] = np.array((apicals, basals, obliques, somas, axons))\n",
    "        axon_to_vector_spine[presyn] = np.array((spines, spine_heads, spine_neck, no_label))\n",
    "        axon_to_vector_spine_joint[presyn] = np.array(apical_joint + basal_joint\n",
    "                                                    + oblique_joint + soma_joint + axon_joint)\n",
    "    \n",
    "        if synapse_total > 3:\n",
    "            threshold_axon_to_vector[presyn] = np.array((apicals, basals, obliques, somas, axons))\n",
    "            threshold_axon_to_vector_spine[presyn] = np.array((spines, spine_heads, spine_neck, no_label))\n",
    "            threshold_axon_to_vector_spine_joint[presyn] = np.array(apical_joint + basal_joint\n",
    "                                                    + oblique_joint + soma_joint + axon_joint)\n",
    "    \n",
    "    #print(axon_to_vector_spine)\n",
    "    #break\n",
    "\n",
    "    \n",
    "axon_compartment_vector = np.array(list(axon_to_vector.values()))\n",
    "threshold_axon_compartment_vector = np.array(list(threshold_axon_to_vector.values()))\n",
    "\n",
    "axon_compartment_vector_spine = np.array(list(axon_to_vector_spine.values()))\n",
    "threshold_axon_compartment_vector_spine = np.array(list(threshold_axon_to_vector_spine.values()))\n",
    "\n",
    "axon_compartment_vector_spine_joint = np.array(list(axon_to_vector_spine_joint.values()))\n",
    "threshold_axon_compartment_vector_spine_joint = np.array(list(threshold_axon_to_vector_spine_joint.values()))\n",
    "\n",
    "#print(axon_compartment_vector_spine)\n",
    "\n",
    "#creates the probability distribution of connections by just dividing by total\n",
    "fingerprints_array = np.zeros(axon_compartment_vector.shape)\n",
    "for i, axon_vector in tqdm(enumerate(axon_compartment_vector)):\n",
    "    fingerprints_array[i] = axon_vector / axon_vector.sum()\n",
    "\n",
    "#creates the probability distribution of connections by just dividing by total\n",
    "threshold_fingerprints_array = np.zeros(threshold_axon_compartment_vector.shape)\n",
    "for i, axon_vector in tqdm(enumerate(threshold_axon_compartment_vector)):\n",
    "    threshold_fingerprints_array[i] = axon_vector / axon_vector.sum()\n",
    "\n",
    "\n",
    "#creates the probability distribution of connections by just dividing by total\n",
    "fingerprints_array_spine = np.zeros(axon_compartment_vector_spine.shape)\n",
    "for i, axon_vector in tqdm(enumerate(axon_compartment_vector_spine)):\n",
    "    fingerprints_array_spine[i] = axon_vector / axon_vector.sum()\n",
    "\n",
    "#creates the probability distribution of connections by just dividing by total\n",
    "threshold_fingerprints_array_spine = np.zeros(threshold_axon_compartment_vector_spine.shape)\n",
    "for i, axon_vector in tqdm(enumerate(threshold_axon_compartment_vector_spine)):\n",
    "    threshold_fingerprints_array_spine[i] = axon_vector / axon_vector.sum()\n",
    "    \n",
    "#creates the probability distribution of connections by just dividing by total\n",
    "fingerprints_array_spine_joint = np.zeros(axon_compartment_vector_spine_joint.shape)\n",
    "for i, axon_vector in tqdm(enumerate(axon_compartment_vector_spine_joint)):\n",
    "    fingerprints_array_spine_joint[i] = axon_vector / axon_vector.sum()\n",
    "\n",
    "#creates the probability distribution of connections by just dividing by total\n",
    "threshold_fingerprints_array_spine_joint = np.zeros(threshold_axon_compartment_vector_spine_joint.shape)\n",
    "for i, axon_vector in tqdm(enumerate(threshold_axon_compartment_vector_spine_joint)):\n",
    "    threshold_fingerprints_array_spine_joint[i] = axon_vector / axon_vector.sum()\n",
    "\n",
    "\n",
    "    \n",
    "threshold_fingerprints_array_spine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize using pandas\n",
    "#print(len(threshold_axon_to_vector.keys()))\n",
    "#print(len(threshold_fingerprints_array_spine))\n",
    "compartment_table = pd.DataFrame(threshold_fingerprints_array,\n",
    "                index=threshold_axon_to_vector.keys(), \n",
    "                columns = [\"apicals\", \"basals\", \"obliques\", \"somas\", \"axons\"])\n",
    "\n",
    "compartment_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getting the axon names for the Fabian analysis\n",
    "apical_frame = compartment_table.loc[compartment_table['apicals'] >= 0.75]\n",
    "apical_frame\n",
    "ids_apical_frame = apical_frame.index\n",
    "print(ids_apical_frame)\n",
    "apical_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize using pandas\n",
    "#print(len(threshold_axon_to_vector.keys()))\n",
    "#print(len(threshold_fingerprints_array_spine))\n",
    "spine_table = pd.DataFrame(threshold_fingerprints_array_spine,\n",
    "                index=threshold_axon_to_vector.keys(), \n",
    "                columns = [\"spines\", \"spine_heads\", \"spine_neck\", \"no_label\"])\n",
    "\n",
    "spine_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize using pandas\n",
    "\n",
    "compartment_label_list = [\"apicals\", \"basals\", \"obliques\", \"somas\", \"axons\"]\n",
    "spine_label_list = [\"spines\", \"spine_heads\", \"spine_neck\", \"no_label\"]\n",
    "joint_label_list = []\n",
    "for c in compartment_label_list:\n",
    "    for s in spine_label_list:\n",
    "        joint_label_list.append(c + \":\" + s)\n",
    "\n",
    "#print(joint_label_list)\n",
    "\n",
    "spine_joint_table = pd.DataFrame(threshold_fingerprints_array_spine_joint,\n",
    "                index=threshold_axon_to_vector_spine_joint.keys(), \n",
    "                columns = joint_label_list)\n",
    "\n",
    "spine_joint_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############  THE CONDENSED JOINT GROUPINGS ####################\n",
    "\n",
    "#combine columns: so only heads and non-heads\n",
    "#create mapping\n",
    "compartment_label_list = [\"apicals\", \"basals\", \"obliques\", \"somas\", \"axons\"]\n",
    "spine_label_list = [\"spine_heads\",\"spine_neck\", \"no_label\"]\n",
    "drop_spine_list = [\"spines\"]\n",
    "\n",
    "\n",
    "mapping_merge = dict()\n",
    "for i in compartment_label_list:\n",
    "    label = i + \":non_head\"\n",
    "    for j in spine_label_list:\n",
    "        if j == \"spine_heads\":\n",
    "            mapping_merge[i + \":\" + j] = i + \":\" + j\n",
    "        else:\n",
    "            mapping_merge[i + \":\" + j] = label\n",
    "        \n",
    "#print(mapping_merge)\n",
    "        \n",
    "\n",
    "spine_head_no_head_table = spine_joint_table.groupby(mapping_merge,axis=1).sum()\n",
    "\n",
    "\n",
    "spine_head_no_head_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compartment_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do thresholding example for high percentage in each category\n",
    "\n",
    "apical_axons = list(compartment_table.loc[compartment_table['apicals'] >= 0.75].index)\n",
    "basal_axons = list(compartment_table.loc[compartment_table['basals'] >= 0.75].index)\n",
    "oblique_axons = list(compartment_table.loc[compartment_table['obliques'] >= 0.75].index)\n",
    "soma_axons = list(compartment_table.loc[compartment_table['somas'] >= 0.75].index)\n",
    "axon_axons = list(compartment_table.loc[compartment_table['axons'] >= 0.75].index)\n",
    "\n",
    "print(type(list(axon_axons)))\n",
    "\n",
    "\n",
    "other_axons = list(set(list(compartment_table.index)).difference(set(apical_axons + basal_axons + oblique_axons + soma_axons + axon_axons)))\n",
    "print(len(compartment_table.index))\n",
    "total_axons = compartment_table.index\n",
    "\n",
    "total_length = len(apical_axons) + len(basal_axons) + len(oblique_axons) + len(soma_axons) + len(axon_axons) + len(other_axons)\n",
    "print(total_length)\n",
    "\n",
    "print(len(other_axons))\n",
    "\n",
    "np.savez(\"compartment_axon_groups.npz\",\n",
    "         apical_axons=apical_axons,\n",
    "        basal_axons=basal_axons,\n",
    "        oblique_axons=oblique_axons,\n",
    "        soma_axons=soma_axons,\n",
    "        axon_axons=axon_axons,\n",
    "        other_axons=other_axons,\n",
    "        total_axons=total_axons)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Label_Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the list\n",
    "axon_groups = [apical_axons,basal_axons,oblique_axons,soma_axons,axon_axons,other_axons,total_axons]\n",
    "axon_names = [\"apical (>0.75)\",\"basal (>0.75)\",\"oblique (>0.75)\",\"soma (>0.75)\",\"axon (>0.75)\",\"other\",\"total\"]\n",
    "\n",
    "#pull down all of the \n",
    "postsyn_list = Label_Table.postsyn.unique()\n",
    "#print(postsyn_list)\n",
    "ax_group_dicts = {}\n",
    "\n",
    "final_table = [None] * (len(postsyn_list)*len(postsyn_list) - len(postsyn_list))*len(axon_names)\n",
    "counter = 0\n",
    "for ax_group,ax_name in tqdm(zip(axon_groups,axon_names)):\n",
    "    postsyn_axon_dict = {k:set() for k in postsyn_list}\n",
    "    \n",
    "    #print(postsyn_axon_dict)\n",
    "    for ax in ax_group:\n",
    "        #get the postsynaptic targets\n",
    "        #print(ax)\n",
    "        #print(Label_Table[\"presyn\"])\n",
    "        mini_table = Label_Table.loc[Label_Table[\"presyn\"] == ax]\n",
    "        if len(mini_table) > 0:\n",
    "            #print(mini_table)\n",
    "            unique_postsyns = mini_table.postsyn.unique()\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        for up in unique_postsyns:\n",
    "            postsyn_axon_dict[up].add(ax)\n",
    "    \n",
    "    ax_group_dicts[ax_name] = postsyn_axon_dict\n",
    "     \n",
    "#ax_group_dicts\n",
    "counter = 0\n",
    "#generate the rows for the table\n",
    "for ax_group_name, ax_group_list in tqdm(ax_group_dicts.items()):\n",
    "    #print(\"ax_group_name = \" + str(ax_group_name))\n",
    "    #print(\"ax_group_list = \" + str(ax_group_list))\n",
    "    for post,post_list in ax_group_list.items():\n",
    "        for pos_post in postsyn_list:\n",
    "            if pos_post == post:\n",
    "                continue\n",
    "            n_seg = len(post_list.intersection(ax_group_dicts[ax_group_name][pos_post]))\n",
    "            #print(n_seg)\n",
    "            final_table[counter] = (post,ax_group_name,pos_post,n_seg)\n",
    "            #print(final_table)\n",
    "            counter += 1\n",
    "            \n",
    "#print(final_table[(len(postsyn_list)*len(postsyn_list) - len(postsyn_list))-1])\n",
    "table_without_functional = pd.DataFrame.from_records(final_table,columns= [\"segment_a\",\"connection\",\"segment_b\",\"n_seg_shared\"])\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table_without_functional.loc[table_without_functional[\"connection\"]==\"other\"]\n",
    "#print(final_table[(len(postsyn_list)*len(postsyn_list) - len(postsyn_list))*5-1])\n",
    "#table_without_functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets up the graphing variables for below\n",
    "is_tuned = 'von_r2>0.05 and von_pvalue<0.05'\n",
    "dori_resolution = np.pi / 2 / 4\n",
    "rad2deg = 180/np.pi\n",
    "bin_edges = np.linspace(0, np.pi,9)\n",
    "be = list(['{:.0f}'.format(ee) for ee in [np.round(e * rad2deg) for e in bin_edges]])\n",
    "bin_labels = list(zip(be[:-1], be[1:]))\n",
    "bin_centers = np.round((bin_edges[1:] + bin_edges[:-1])/2 * rad2deg, decimals=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_rel = nda.VonMises() & is_tuned\n",
    "best_r2 = dj.U('segment_id').aggr(tuned_rel, best_r2='max(von_r2)')\n",
    "pref_rel = (tuned_rel * best_r2) & 'von_r2 >= best_r2'\n",
    "pref_rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add on the functional group preferences\n",
    "#restricts and appends the orientation\n",
    "\n",
    "#query that will get the functional data\n",
    "rel2 = pref_rel.proj('scan_idx',ori_1='von_pref', segment_a='segment_id') \n",
    "functional_id,ori,scan_idx = rel2.fetch(\"segment_a\",\"ori_1\",\"scan_idx\") \n",
    "\n",
    "#create functional lookup table\n",
    "func_lookup = dict()\n",
    "for i,id in enumerate(functional_id):\n",
    "    func_lookup[id] = [ori[i],scan_idx[i]]\n",
    "\n",
    "#iterate through the list and only keep those whose segments are in functional list and then append the data\n",
    "final_list_with_functional = []#[\"None\"] * len(final_table)\n",
    "counter = 0\n",
    "\n",
    "dummy=2\n",
    "segmentation=1\n",
    "\n",
    "\n",
    "for ll in final_table: \n",
    "    if ll[0] in functional_id and ll[2] in functional_id:\n",
    "        final_list_with_functional.append((ll[0],ll[1],ll[2],ll[3],\n",
    "                                               int(dummy),int(segmentation),int(func_lookup[ll[0]][1]),func_lookup[ll[0]][0],\n",
    "                                              func_lookup[ll[2]][0]))\n",
    "        counter += 1\n",
    "\n",
    "print(len(final_list_with_functional))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(final_table[(len(postsyn_list)*len(postsyn_list) - len(postsyn_list))-1])\n",
    "table_with_functional = pd.DataFrame.from_records(final_list_with_functional,columns= \n",
    "                [\"segment_a\",\"connection\",\"segment_b\",\"n_seg_shared\",\"dummy\",\"segmentation\",\"scan_idx\",\n",
    "                        \"ori_1\",\"ori_2\"])\n",
    "\n",
    "#table_with_functional.loc[table_with_functional[\"n_seg_shared\"]>0]\n",
    "###########--------------------TABLE TO USE FOR THE FABIAN ANALYSIS------------------##########\n",
    "table_with_functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_with_functional.to_pickle(\"shared_connections.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# import tsne\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import gaussian_kde\n",
    "from numpy import linalg\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "#Try an example clustering\n",
    "input_axons = threshold_fingerprints_array_spine_joint \n",
    "clusterer_input = input_axons\n",
    "clusterer = KMeans(n_clusters=8, n_init=10, random_state=0)\n",
    "clusterer.fit(clusterer_input)\n",
    "clusterer.labels_\n",
    "\n",
    "scores_per_cluster = np.zeros((input_axons.shape[0], clusterer.n_clusters))\n",
    "for i, feature_vector in enumerate(input_axons):\n",
    "    for j, center in enumerate(clusterer.cluster_centers_):\n",
    "        scores_per_cluster[i, j] = np.linalg.norm(center-feature_vector)\n",
    "scores_per_cluster = np.exp(-scores_per_cluster)\n",
    "# scores_per_cluster = scores_per_cluster\n",
    "groupings = clusterer.labels_#np.argmin(scores_per_cluster, axis=1)\n",
    "groupings_dict = dict()\n",
    "for i in np.unique(groupings):\n",
    "    groupings_dict[i] = list()\n",
    "for i, cluster in enumerate(groupings):\n",
    "    groupings_dict[cluster].append(i)\n",
    "mean_scores_per_group = np.zeros((clusterer.n_clusters, clusterer.n_clusters))\n",
    "for i, axons in groupings_dict.items():\n",
    "    group_scores = scores_per_cluster[np.array(axons)]\n",
    "    for j, row in enumerate(group_scores):\n",
    "        group_scores[j] /= row.sum()\n",
    "    for j, column in enumerate(group_scores.T):\n",
    "        mean_scores_per_group[i, j] = column.mean()\n",
    "for i, row in enumerate(mean_scores_per_group):\n",
    "    print(\"Expected Cluster:\", i, \"-> index of max:\", np.argmax(row))\n",
    "sns.heatmap(mean_scores_per_group)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print the average compartment level distribution of each grouping\n",
    "import numpy as np\n",
    "cluster_output = clusterer.labels_\n",
    "\n",
    "group_distributions = dict()\n",
    "\n",
    "for i,ax in enumerate(input_axons):\n",
    "    #get grouping\n",
    "    group = cluster_output[i]\n",
    "    if group not in group_distributions:\n",
    "        group_distributions[group] = list()\n",
    "        \n",
    "    group_distributions[group].append(ax)\n",
    "\n",
    "print()\n",
    "#find the average distribution for each group\n",
    "average_dist = [np.matrix(k).mean(axis=0) for i,k in group_distributions.items()]\n",
    "variance_dist = [np.matrix(k).var(axis=0) for i,k in group_distributions.items()]\n",
    "group_population = [len(k) for i,k in group_distributions.items()]\n",
    "\n",
    "print(group_population)\n",
    "print(\"average_dist = \")\n",
    "print(\"apicals,     basals,   obliques,    somas,  axons\")\n",
    "for i in average_dist:\n",
    "    print(i)\n",
    "    \n",
    "\n",
    "print(\"variance_dist = \")\n",
    "print(\"apicals,     basals,   obliques,    somas,  axons\")\n",
    "for i in variance_dist:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fabian Analysis Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funconnect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
